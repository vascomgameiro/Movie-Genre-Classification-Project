{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurate Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = \"C:/Users/vasco/repos/Natural-Language\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from src.dataset import read_data\n",
    "\n",
    "path = \"data/raw/train.txt\"\n",
    "columns = [\"title\", \"from\", \"genre\", \"director\", \"description\"]\n",
    "\n",
    "df = read_data(path, columns)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Impressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "print(df.info())\n",
    "print(\"*\" * 20)\n",
    "repeated_titles = df[\"title\"].value_counts()[df[\"title\"].value_counts() > 1].head(5)\n",
    "print(repeated_titles)\n",
    "print(\"*\" * 20)\n",
    "popular_directors = df[\"director\"].value_counts().head(5)\n",
    "print(popular_directors)\n",
    "print(\"*\" * 20)\n",
    "print(df[\"from\"].value_counts().head(5))\n",
    "print(\"*\" * 20)\n",
    "print(df[\"genre\"].value_counts())\n",
    "print(\"*\" * 20)\n",
    "print(\"Number of duplicates:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(keep=False)].sort_values(\"title\").head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from src.plots import (\n",
    "    plot_movie_data,\n",
    "    plot_stopword_frequency,\n",
    "    get_text_statistics,\n",
    "    plot_histograms,\n",
    "    plot_boxplots,\n",
    "    plot_pca_tfidf,\n",
    "    plot_correlation_matrix,\n",
    "    plot_pca,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_movie_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stopword_frequency(df, \"description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stopword_frequency(df, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = get_text_statistics(df, \"description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_description_df, pca_description = plot_pca_tfidf(df, \"description\", \"genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_title_df, pca_title = plot_pca_tfidf(df, \"title\", \"genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(stats)\n",
    "pca_stats_df, pca_stats = plot_pca(stats, df[\"genre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"genre\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate Candidates & Cleaning Directors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from src.director_encoder import (\n",
    "    filter_duplicate_descriptions,\n",
    "    find_similar_descriptions,\n",
    "    print_differences,\n",
    "    get_encoding_map,\n",
    "    encode_directors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_duplicate_descriptions(train_df, \"description\", \"title\").head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs = find_similar_descriptions(train_df, \"description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_differences(train_df, similar_pairs, \"title\")\n",
    "print(\"\\n\")\n",
    "print_differences(train_df, similar_pairs, \"director\")\n",
    "print(\"\\n\")\n",
    "print_differences(train_df, similar_pairs, \"genre\")\n",
    "print(\"\\n\")\n",
    "print_differences(train_df, similar_pairs, \"from\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = get_encoding_map(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = encode_directors(train_df, \"train\", mapping)\n",
    "test_df = encode_directors(test_df, \"test\", mapping)\n",
    "\n",
    "train_df.drop(\"director\", axis=1, inplace=True)\n",
    "test_df.drop(\"director\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embedding import process_and_save_in_chunks\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Example of how embeddings were generated, in practice we ran the embedding.py file\n",
    "model = SentenceTransformer(\"avsolatorio/GIST-small-Embedding-v0\")\n",
    "output_path = \"data/processed/processed_embeddings_train.csv\"\n",
    "process_and_save_in_chunks(train_df, \"description\", model, chunk_size=300, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "embedding_train_path = \"data/processed/processed_embeddings_train.csv\"\n",
    "embedding_train_df = pd.read_csv(embedding_train_path)\n",
    "\n",
    "embedding_train_df = pd.concat([train_df[\"genre\"], embedding_train_df], axis=1)\n",
    "embedding_train_df.drop_duplicates(inplace=True)\n",
    "embedding_train_df.reset_index(drop=True, inplace=True)\n",
    "embedding_train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data/processed/processed_embeddings_test.csv\"\n",
    "process_and_save_in_chunks(test_df, \"description\", model, chunk_size=300, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_test_path = \"data/processed/processed_embeddings_test.csv\"\n",
    "embedding_test_df = pd.read_csv(embedding_test_path)\n",
    "\n",
    "embedding_test_df = pd.concat([test_df[\"genre\"], embedding_test_df], axis=1)\n",
    "embedding_test_df.reset_index(drop=True, inplace=True)\n",
    "embedding_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_aux = embedding_train_df.drop(\"genre\", inplace=False, axis=1)\n",
    "pca_embedding_df, pca_embedding = plot_pca(embeddings_aux, embedding_train_df[\"genre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from src.preprocessing import preprocess_sentence\n",
    "\n",
    "# Expand contractions, extract noun-phrases, tokenize and lemmatize (optionally remove stopwords)\n",
    "train_df[\"title\"] = train_df[\"title\"].apply(preprocess_sentence)\n",
    "train_df[\"description\"] = train_df[\"description\"].apply(preprocess_sentence)\n",
    "\n",
    "test_df[\"title\"] = test_df[\"title\"].apply(preprocess_sentence)\n",
    "test_df[\"description\"] = test_df[\"description\"].apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from src.features import REGION_MAP, select_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"region\"] = train_df[\"from\"].replace(REGION_MAP)\n",
    "test_df[\"region\"] = test_df[\"from\"].replace(REGION_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection (Log Ratio Analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from src.logratioanalysis import LogRatioAnalysis, plot_scree_subplots_for_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logratio_title = LogRatioAnalysis(train_df, \"title\", \"genre\")\n",
    "logratio_description = LogRatioAnalysis(train_df, \"description\", \"genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scree_subplots_for_genres(logratio_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scree_subplots_for_genres(logratio_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_tokens = logratio_description.feature_selection(25000)\n",
    "title_tokens = logratio_title.feature_selection(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"selected_description\"] = train_df[\"description\"].apply(select_tokens, selected_tokens=description_tokens)\n",
    "test_df[\"selected_description\"] = test_df[\"description\"].apply(select_tokens, selected_tokens=description_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from skrub import SelectCols, SimilarityEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from src.plots import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df[\"genre\"])\n",
    "train_df[\"genre\"] = label_encoder.transform(train_df[\"genre\"])\n",
    "decoded_class_names = label_encoder.inverse_transform(range(len(label_encoder.classes_)))\n",
    "\n",
    "X_train = train_df.drop(\"genre\", axis=1)\n",
    "y_train = train_df[\"genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"genre\"] = label_encoder.transform(test_df[\"genre\"])\n",
    "\n",
    "X_test = test_df.drop(\"genre\", axis=1)\n",
    "y_test = test_df[\"genre\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1: HistGradientBoostingClassifier w/ SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline_1 = make_union(\n",
    "    make_pipeline(\n",
    "        ColumnSelector(\"title\", drop_axis=True),\n",
    "        TfidfVectorizer(),\n",
    "        TruncatedSVD(),\n",
    "    ),\n",
    "    make_pipeline(\n",
    "        ColumnSelector(\"description\", drop_axis=True),\n",
    "        TfidfVectorizer(),\n",
    "        TruncatedSVD(),\n",
    "    ),\n",
    "    make_pipeline(SelectCols(\"region\"), SimilarityEncoder()),\n",
    "    make_pipeline(SelectCols(\"from\"), OneHotEncoder(sparse_output=False)),\n",
    "    make_pipeline(SelectCols(\"encoded_director\")),\n",
    ")\n",
    "\n",
    "pipeline_1 = make_pipeline(text_pipeline_1, HistGradientBoostingClassifier())\n",
    "pipeline_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"histgradientboostingclassifier__learning_rate\": np.logspace(-3, 0, 100),\n",
    "    \"histgradientboostingclassifier__max_iter\": np.arange(100, 500, 50),\n",
    "    \"histgradientboostingclassifier__max_depth\": np.arange(3, 15),\n",
    "    \"histgradientboostingclassifier__min_samples_leaf\": np.arange(1, 51, 5),\n",
    "    \"histgradientboostingclassifier__max_leaf_nodes\": np.arange(10, 301, 10),\n",
    "    \"histgradientboostingclassifier__l2_regularization\": np.logspace(-4, 0, 100),\n",
    "    \"histgradientboostingclassifier__scoring\": [\"accuracy\"],\n",
    "    \"featureunion__pipeline-1__truncatedsvd__n_components\": np.arange(200, 400, 50),\n",
    "    \"featureunion__pipeline-2__truncatedsvd__n_components\": np.arange(4000, 6000, 500),\n",
    "    \"featureunion__pipeline-5__truncatedsvd__n_components\": np.arange(200, 400, 50),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_1 = RandomizedSearchCV(\n",
    "    pipeline_1,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,\n",
    "    cv=4,\n",
    "    scoring=\"accuracy\",\n",
    "    random_state=42,\n",
    "    verbose=4,\n",
    "    error_score=\"raise\",\n",
    ")\n",
    "random_search_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search_1.best_params_\n",
    "best_score = random_search_1.best_score_\n",
    "best_model = random_search_1.best_estimator_\n",
    "\n",
    "print(\"\\n Best Parameters:\", best_params)\n",
    "print(\"\\n Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use best metrics to predict on test set\n",
    "pipeline_1.set_params(**best_params)\n",
    "pipeline_1.fit(X_train, y_train)\n",
    "y_pred = pipeline_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm,decoded_class_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: SVM w/o feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline_2 = make_union(\n",
    "    make_pipeline(\n",
    "        ColumnSelector(\"title\", drop_axis=True),\n",
    "        TfidfVectorizer(),\n",
    "    ),\n",
    "    make_pipeline(\n",
    "        ColumnSelector(\"description\", drop_axis=True),\n",
    "        TfidfVectorizer(),\n",
    "    ),\n",
    "    make_pipeline(SelectCols(\"region\"), SimilarityEncoder()),\n",
    "    make_pipeline(SelectCols(\"from\"), OneHotEncoder(sparse_output=False)),\n",
    "    make_pipeline(SelectCols(\"encoded_director\")),\n",
    ")\n",
    "\n",
    "pipeline_2 = make_pipeline(text_pipeline_2, SVC())\n",
    "pipeline_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"svc__C\": np.logspace(-3, 3, 100),\n",
    "    \"svc__kernel\": [\"poly\", \"sigmoid\", \"rbf\", \"linear\"],\n",
    "    \"svc__gamma\": [\"scale\", \"auto\"] + list(np.logspace(-4, 1, 100)),\n",
    "    \"svc__degree\": np.arange(2, 6),\n",
    "    \"svc__coef0\": np.linspace(-1, 1, 100),\n",
    "    \"svc__class_weight\": [\"balanced\", None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_2 = RandomizedSearchCV(\n",
    "    pipeline_2,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    cv=4,\n",
    "    scoring=\"accuracy\",\n",
    "    random_state=42,\n",
    "    verbose=4,\n",
    "    error_score=\"raise\",\n",
    ")\n",
    "random_search_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search_2.best_params_\n",
    "best_score = random_search_2.best_score_\n",
    "best_model = random_search_2.best_estimator_\n",
    "\n",
    "print(\"\\nBest Parameters:\", best_params)\n",
    "print(\"\\nBest Score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters: {\n",
    "    \n",
    "    'svc__C': np.float64(3.9676050770529883), \n",
    "\n",
    "    'svc__coef0': np.float64(0.3991305878561679), \n",
    "    \n",
    "    'svc__degree': 4, 'svc__gamma': 'auto',\n",
    "    \n",
    "    'svc__kernel': 'linear'}\n",
    "\n",
    "Best Score: 0.6512520009226962"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use best metrics to predict on test set\n",
    "pipeline_2.set_params(**best_params)\n",
    "pipeline_2.fit(X_train, y_train)\n",
    "y_pred = pipeline_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, decoded_class_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: SVM w/ Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline_3 = make_union(\n",
    "    make_pipeline(\n",
    "        ColumnSelector(\"title\", drop_axis=True),\n",
    "        TfidfVectorizer(ngram_range=(2, 4)),\n",
    "    ),\n",
    "    make_pipeline(\n",
    "        ColumnSelector(\"selected_description\", drop_axis=True),\n",
    "        TfidfVectorizer(),\n",
    "    ),\n",
    "    make_pipeline(SelectCols(\"region\"), SimilarityEncoder()),\n",
    "    make_pipeline(SelectCols(\"from\"), OneHotEncoder(sparse_output=False)),\n",
    "    make_pipeline(SelectCols(\"encoded_director\")),\n",
    ")\n",
    "\n",
    "pipeline_3 = make_pipeline(text_pipeline_3, SVC())\n",
    "pipeline_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_3 = RandomizedSearchCV(\n",
    "    pipeline_3,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    cv=4,\n",
    "    scoring=\"accuracy\",\n",
    "    random_state=42,\n",
    "    verbose=4,\n",
    "    error_score=\"raise\",\n",
    ")\n",
    "random_search_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search_3.best_params_\n",
    "best_score = random_search_3.best_score_\n",
    "best_model = random_search_3.best_estimator_\n",
    "\n",
    "print(\"\\nBest Parameters:\", best_params)\n",
    "print(\"\\nBest Score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters: {\n",
    "    \n",
    "    'svc__C': np.float64(3.9676050770529883), \n",
    "\n",
    "    'svc__coef0': np.float6(03991305878561679), \n",
    "\n",
    "    'svc__degree': 4, \n",
    "\n",
    "    'svc__gamma': 'auto', \n",
    "\n",
    "    'svc__kernel': 'linear'}\n",
    "\n",
    "Best Score: 0.6364193794168839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use best metrics to predict on test set\n",
    "pipeline_3.set_params(**best_params)\n",
    "pipeline_3.fit(X_train, y_train)\n",
    "y_pred = pipeline_3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, decoded_class_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(train_df[\"genre\"])\n",
    "embedding_train_df[\"genre\"] = label_encoder.transform(embedding_train_df[\"genre\"])\n",
    "decoded_class_names = label_encoder.inverse_transform(range(len(label_encoder.classes_)))\n",
    "\n",
    "X_train = embedding_train_df.drop(\"genre\", axis=1)\n",
    "y_train = embedding_train_df[\"genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_test_df[\"genre\"] = label_encoder.transform(embedding_test_df[\"genre\"])\n",
    "\n",
    "X_test = embedding_test_df.drop(\"genre\", axis=1)\n",
    "y_test = embedding_test_df[\"genre\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 4: SVM w/ Embeddings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"C\": np.logspace(-3, 3, 100),\n",
    "    \"kernel\": [\"poly\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"] + list(np.logspace(-4, 1, 100)),\n",
    "    \"degree\": np.arange(2, 6),\n",
    "    \"coef0\": np.linspace(-1, 1, 100),\n",
    "    \"class_weight\": [\"balanced\", None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_4 = RandomizedSearchCV(\n",
    "    SVC(),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=4,\n",
    "    verbose=4,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "random_search_4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search_4.best_params_\n",
    "best_score = random_search_4.best_score_\n",
    "best_model = random_search_4.best_estimator_\n",
    "\n",
    "print(\"\\nBest Parameters:\", best_params)\n",
    "print(\"\\nBest Score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters: \n",
    "{'kernel': 'poly',\n",
    "\n",
    "'gamma': 7.054802310718645,\n",
    "\n",
    "'degree': 3,\n",
    "\n",
    "'coef0': -0.050505050505050386,\n",
    "\n",
    "'class_weight': None,\n",
    "\n",
    "'C': 0.0026560877829466868}\n",
    "\n",
    "Best Score: 0.6867000556483027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use best metrics to predict on test set\n",
    "svc = SVC()\n",
    "svc.set_params(**best_params)\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracy: 0.676214196762142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, decoded_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 5: HistGradientBoosting w/ Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"learning_rate\": np.logspace(-3, 0, 100),\n",
    "    \"max_iter\": np.arange(100, 500, 50),\n",
    "    \"max_depth\": np.arange(3, 15),\n",
    "    \"min_samples_leaf\": np.arange(1, 51, 5),\n",
    "    \"max_leaf_nodes\": np.arange(10, 301, 10),\n",
    "    \"l2_regularization\": np.logspace(-4, 0, 100),\n",
    "    \"scoring\": [\"accuracy\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_5 = RandomizedSearchCV(\n",
    "    HistGradientBoostingClassifier(),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=4,\n",
    "    verbose=4,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "random_search_5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search_5.best_params_\n",
    "best_score = random_search_5.best_score_\n",
    "best_model = random_search_5.best_estimator_\n",
    "\n",
    "print(\"\\nBest Parameters:\", best_params)\n",
    "print(\"\\nBest Score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters found: {\n",
    "    \n",
    "    'scoring': 'accuracy', \n",
    "\n",
    "    'min_samples_leaf': 6, \n",
    "\n",
    "    'max_leaf_nodes': 50, \n",
    "\n",
    "    'max_iter': 350, \n",
    "\n",
    "    'max_depth': 6, \n",
    "\n",
    "    'learning_rate': 0.1, \n",
    "\n",
    "    'l2_regularization': 0.0004430621457583882, \n",
    "    }\n",
    "Best score found: 0.671883263271661"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use best metrics to predict on test set\n",
    "hgb = HistGradientBoostingClassifier()\n",
    "hgb.set_params(**best_params)\n",
    "hgb.fit(X_train, y_train)\n",
    "y_pred = hgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, decoded_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = \"data/raw/test_no_labels.txt\"\n",
    "columns_test = [\"title\", \"from\", \"director\", \"description\"]\n",
    "\n",
    "df_no_labels_test = read_data(path_test, columns_test)\n",
    "df_no_labels_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"data/raw/train.txt\"\n",
    "columns = [\"title\", \"from\", \"genre\", \"director\", \"description\"]\n",
    "\n",
    "df = read_data(path, columns)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_description = df[\"description\"].dropna().unique()\n",
    "test_description = df_no_labels_test[\"description\"].dropna().unique()\n",
    "\n",
    "overlapping_descriptions = set(train_description).intersection(set(test_description))\n",
    "print(f\"{len(overlapping_descriptions)} overlapping descriptions found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_based_dict = df[df[\"description\"].isin(overlapping_descriptions)].set_index(\"description\")[\"genre\"].to_dict()\n",
    "\n",
    "\n",
    "def rule_based_prediction(description):\n",
    "    if description in rule_based_dict:\n",
    "        return rule_based_dict[description]\n",
    "    return None\n",
    "\n",
    "\n",
    "df_no_labels_test[\"rule_based_prediction\"] = df_no_labels_test[\"description\"].apply(rule_based_prediction)\n",
    "\n",
    "df_no_labels_test[df_no_labels_test[\"description\"].isin(overlapping_descriptions)].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data/processed/processed_embeddings_no_labels_test.csv\"\n",
    "process_and_save_in_chunks(df_no_labels_test, \"description\", model, chunk_size=300, output_path=output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
