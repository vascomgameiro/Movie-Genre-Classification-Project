{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = \"C:/Users/vasco/repos/Natural-Language\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from src.dataset import get_data\n",
    "\n",
    "path = \"data/raw/train.txt\"\n",
    "columns = [\"Title\", \"From\", \"Genre\", \"Director\", \"Description\"]\n",
    "\n",
    "df = get_data(path, columns)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "print(df.info())\n",
    "print(\"*\" * 20)\n",
    "repeated_titles = df[\"Title\"].value_counts()[df[\"Title\"].value_counts() > 1].head(5)\n",
    "print(repeated_titles)\n",
    "print(\"*\" * 20)\n",
    "popular_directors = df[\"Director\"].value_counts().head(5)\n",
    "print(popular_directors)\n",
    "print(\"*\" * 20)\n",
    "print(df[\"From\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Duplicated Movies (Check titles, check Description)\n",
    "- Unknowns in Director\n",
    "- Weird symbols in some titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot styles\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1. Distribution of Genres\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(y=\"Genre\", data=df, order=df[\"Genre\"].value_counts().index)\n",
    "plt.title(\"Distribution of Movie Genres\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Distribution of Directors (Top 10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_directors = df[\"Director\"].value_counts().head(10)\n",
    "sns.barplot(x=top_directors.values, y=top_directors.index)\n",
    "plt.title(\"Top 10 Most Popular Directors\")\n",
    "plt.xlabel(\"Number of Movies\")\n",
    "plt.ylabel(\"Director\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Genre vs Director Count (Top 10 Directors)\n",
    "top_10_directors = df[\"Director\"].value_counts().head(10).index\n",
    "filtered_df = df[df[\"Director\"].isin(top_10_directors)]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.countplot(y=\"Director\", hue=\"Genre\", data=filtered_df)\n",
    "plt.title(\"Top 10 Directors by Genre\")\n",
    "plt.xlabel(\"Number of Movies\")\n",
    "plt.ylabel(\"Director\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to compute Jaccard similarity between two sets of words\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "# Tokenization function for Jaccard\n",
    "def tokenize_description(description):\n",
    "    return set(description.lower().split())\n",
    "\n",
    "\n",
    "# Load and clean the descriptions\n",
    "descriptions = df[\"Description\"].fillna(\"\")\n",
    "\n",
    "# Step 1: Compute cosine similarity using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(descriptions)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Threshold for cosine similarity\n",
    "cosine_threshold = 0.8\n",
    "candidate_pairs = []\n",
    "\n",
    "# Step 2: Filter candidates using cosine similarity\n",
    "for i in range(cosine_sim.shape[0]):\n",
    "    for j in range(i + 1, cosine_sim.shape[0]):\n",
    "        if cosine_sim[i, j] >= cosine_threshold:\n",
    "            candidate_pairs.append((i, j, cosine_sim[i, j]))\n",
    "\n",
    "# Step 3: Apply Jaccard similarity to the candidate pairs\n",
    "final_similar_pairs = []\n",
    "jaccard_threshold = 0.7  # Adjust the threshold as needed\n",
    "\n",
    "for i, j, cos_sim in candidate_pairs:\n",
    "    set1 = tokenize_description(df.loc[i, \"Description\"])\n",
    "    set2 = tokenize_description(df.loc[j, \"Description\"])\n",
    "    jac_sim = jaccard_similarity(set1, set2)\n",
    "\n",
    "    if jac_sim >= jaccard_threshold:\n",
    "        final_similar_pairs.append((i, j, cos_sim, jac_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Different Titles: \\n\")\n",
    "for i, j, cos_sim, jac_sim in final_similar_pairs:\n",
    "    title_i = df.loc[i, \"Title\"]\n",
    "    title_j = df.loc[j, \"Title\"]\n",
    "    if title_i != title_j:\n",
    "        print(\n",
    "            f\"Different Titles: {title_i} and {title_j}, with cosine similarity of {cos_sim:.4f} and Jaccard similarity of {jac_sim:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"Title\"] == \"Lakshmi Putrudu\") | (df[\"Title\"] == \"Vambu Sandai\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Different Titles: \\n\")\n",
    "for i, j, cos_sim, jac_simin in final_similar_pairs:\n",
    "    director_i = df.loc[i, \"Director\"]\n",
    "    director_j = df.loc[j, \"Director\"]\n",
    "    if director_i != director_j:\n",
    "        print(\n",
    "            f\"Different Directors: {director_i} and {director_j}, with cosine similarity of {cos_sim:.4f} and Jaccard similarity of {jac_sim:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Different Genre: \\n\")\n",
    "for i, j, cos_sim, jac_simin in final_similar_pairs:\n",
    "    genre_i = df.loc[i, \"Genre\"]\n",
    "    genre_j = df.loc[j, \"Genre\"]\n",
    "    if genre_i != genre_j:\n",
    "        print(\n",
    "            f\"Different Genre: {genre_i} and {genre_j}, with cosine similarity of {cos_sim:.4f} and Jaccard similarity of {jac_sim:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import numpy as np\n",
    "\n",
    "from src.dataset import preprocess_sentence\n",
    "\n",
    "df.loc[:, \"Director\"] = df.loc[:, \"Director\"].replace(\"Unknown\", np.nan)\n",
    "df[\"Title\"] = df[\"Title\"].apply(preprocess_sentence)\n",
    "df[\"Description\"] = df[\"Description\"].apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "df[\"Region\"] = df[\"From\"].map(\n",
    "    {\n",
    "        \"American\": \"Western\",\n",
    "        \"British\": \"Western\",\n",
    "        \"Canadian\": \"Western\",\n",
    "        \"Australian\": \"Western\",\n",
    "        \"Bollywood\": \"South Asian\",\n",
    "        \"Telugu\": \"South Asian\",\n",
    "        \"Tamil\": \"South Asian\",\n",
    "        \"Malayalam\": \"South Asian\",\n",
    "        \"Bengali\": \"South Asian\",\n",
    "        \"Kannada\": \"South Asian\",\n",
    "        \"Marathi\": \"South Asian\",\n",
    "        \"Punjabi\": \"South Asian\",\n",
    "        \"Assamese\": \"South Asian\",\n",
    "        \"Chinese\": \"East Asian\",\n",
    "        \"Japanese\": \"East Asian\",\n",
    "        \"South_Korean\": \"East Asian\",\n",
    "        \"Hong Kong\": \"East Asian\",\n",
    "        \"Filipino\": \"Southeast Asian\",\n",
    "        \"Bangladeshi\": \"South Asian\",\n",
    "        \"Russian\": \"European\",\n",
    "        \"Turkish\": \"Middle Eastern\",\n",
    "        \"Egyptian\": \"Middle Eastern\",\n",
    "        \"Malaysian\": \"Southeast Asian\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from joblib import Memory\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from skrub import SelectCols, SimilarityEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "class SelectColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Ensure that X is a DataFrame and return the selected columns\n",
    "        cpy_df = X[self.columns].copy()\n",
    "        return cpy_df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "\n",
    "memory = Memory(location=\"cache_directory\")\n",
    "text_pipeline = make_union(\n",
    "    make_pipeline(\n",
    "        SelectColumnsTransformer(\"Title\"),\n",
    "        TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False),\n",
    "        StandardScaler(with_mean=False),\n",
    "        TruncatedSVD(n_components=50),\n",
    "    ),\n",
    "    make_pipeline(\n",
    "        SelectColumnsTransformer(\"Description\"),\n",
    "        TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False),\n",
    "        StandardScaler(with_mean=False),\n",
    "        TruncatedSVD(n_components=100),\n",
    "    ),\n",
    "    make_pipeline(SelectCols(\"Region\"), SimilarityEncoder()),\n",
    "    make_pipeline(SelectCols(\"From\"), OneHotEncoder()),\n",
    ")\n",
    "\n",
    "pipeline = make_pipeline(text_pipeline, XGBClassifier())\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df[\"Genre\"] = label_encoder.fit_transform(df[\"Genre\"])\n",
    "\n",
    "X_train = df.drop(\"Genre\", axis=1)\n",
    "y_train = df[\"Genre\"]\n",
    "\n",
    "param_distributions = {\n",
    "    \"xgbclassifier__n_estimators\": randint(100, 500),\n",
    "    \"xgbclassifier__learning_rate\": loguniform(1e-3, 1),  # Log scale for learning rate\n",
    "    \"xgbclassifier__max_depth\": randint(3, 10),\n",
    "    \"xgbclassifier__subsample\": uniform(0.3, 0.7),\n",
    "    \"xgbclassifier__min_child_weight\": randint(1, 10),  # Replaces min_samples_split\n",
    "    \"xgbclassifier__gamma\": uniform(0, 5),  # Regularization term\n",
    "    \"xgbclassifier__colsample_bytree\": uniform(0.3, 0.7),  # Feature subsampling\n",
    "    \"featureunion__pipeline-1__truncatedsvd__n_components\": randint(50, 200),\n",
    "    \"featureunion__pipeline-2__truncatedsvd__n_components\": randint(50, 200),\n",
    "}\n",
    "\n",
    "# Set up the RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)\n",
    "print(best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
