{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = \"C:/Users/vasco/repos/Natural-Language\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from src.dataset import read_data\n",
    "\n",
    "path = \"data/raw/train.txt\"\n",
    "columns = [\"Title\", \"From\", \"Genre\", \"Director\", \"Description\"]\n",
    "\n",
    "df = read_data(path, columns)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "print(df.info())\n",
    "print(\"*\" * 20)\n",
    "repeated_titles = df[\"Title\"].value_counts()[df[\"Title\"].value_counts() > 1].head(5)\n",
    "print(repeated_titles)\n",
    "print(\"*\" * 20)\n",
    "popular_directors = df[\"Director\"].value_counts().head(5)\n",
    "print(popular_directors)\n",
    "print(\"*\" * 20)\n",
    "print(df[\"From\"].value_counts().head(5))\n",
    "print(\"*\" * 20)\n",
    "print(\"Number of duplicates:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot styles\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# 1. Distribution of Genres\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(y=\"Genre\", data=df, order=df[\"Genre\"].value_counts().index)\n",
    "plt.title(\"Distribution of Movie Genres\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Distribution of Directors (Top 10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_directors = df[\"Director\"].value_counts().head(10)\n",
    "sns.barplot(x=top_directors.values, y=top_directors.index)\n",
    "plt.title(\"Top 10 Most Popular Directors\")\n",
    "plt.xlabel(\"Number of Movies\")\n",
    "plt.ylabel(\"Director\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Genre vs Director Count (Top 10 Directors)\n",
    "top_10_directors = df[\"Director\"].value_counts().head(10).index\n",
    "filtered_df = df[df[\"Director\"].isin(top_10_directors)]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.countplot(y=\"Director\", hue=\"Genre\", data=filtered_df)\n",
    "plt.title(\"Top 10 Directors by Genre\")\n",
    "plt.xlabel(\"Number of Movies\")\n",
    "plt.ylabel(\"Director\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def tokenize_description(description):\n",
    "    return set(description.lower().split())\n",
    "\n",
    "\n",
    "descriptions = df[\"Description\"].fillna(\"\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(descriptions)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "cosine_threshold = 0.8\n",
    "candidate_pairs = []\n",
    "\n",
    "for i in range(cosine_sim.shape[0]):\n",
    "    for j in range(i + 1, cosine_sim.shape[0]):\n",
    "        if cosine_sim[i, j] >= cosine_threshold:\n",
    "            candidate_pairs.append((i, j, cosine_sim[i, j]))\n",
    "\n",
    "final_similar_pairs = []\n",
    "jaccard_threshold = 0.7\n",
    "\n",
    "for i, j, cos_sim in candidate_pairs:\n",
    "    set1 = tokenize_description(df.loc[i, \"Description\"])\n",
    "    set2 = tokenize_description(df.loc[j, \"Description\"])\n",
    "    jac_sim = jaccard_similarity(set1, set2)\n",
    "\n",
    "    if jac_sim >= jaccard_threshold:\n",
    "        final_similar_pairs.append((i, j, cos_sim, jac_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Different Titles: \\n\")\n",
    "for i, j, cos_sim, jac_sim in final_similar_pairs:\n",
    "    title_i = df.loc[i, \"Title\"]\n",
    "    title_j = df.loc[j, \"Title\"]\n",
    "    if title_i != title_j:\n",
    "        print(f\"{title_i} ({i}) and {title_j} ({j}) : (Cosine {cos_sim:.4f}, Jaccard  {jac_sim:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Different Directors: \\n\")\n",
    "for i, j, cos_sim, jac_sim in final_similar_pairs:\n",
    "    director_i = df.loc[i, \"Director\"]\n",
    "    director_j = df.loc[j, \"Director\"]\n",
    "    if director_i != director_j:\n",
    "        print(f\"{director_i} ({i}) and {director_j} ({j}) : (Cosine {cos_sim:.4f}, Jaccard {jac_sim:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Different Genre: \\n\")\n",
    "for i, j, cos_sim, jac_sim in final_similar_pairs:\n",
    "    genre_i = df.loc[i, \"Genre\"]\n",
    "    genre_j = df.loc[j, \"Genre\"]\n",
    "    if genre_i != genre_j:\n",
    "        print(f\" {genre_i} ({i}) and {genre_j} ({j}) : (Cosine {cos_sim:.4f}, Jaccard {jac_sim:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Utility function to clean a director's name by removing spaces\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_director_name(name: str):\n",
    "    # Remove spaces, hyphens, periods, and convert to lowercase\n",
    "    name = name.lower().replace(\" \", \"\").replace(\"-\", \"\")\n",
    "    return re.sub(r\"\\.\", \"\", name)  # Remove periods as well\n",
    "\n",
    "\n",
    "# Step 2: Create a mapping of cleaned names to the original names\n",
    "def create_name_map(df: pd.DataFrame):\n",
    "    name_map = defaultdict(set)  # Use a list to account for potential multiple matches\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        director_list = [name.strip() for name in row[\"Director\"].split(\",\")]\n",
    "\n",
    "        for director in director_list:\n",
    "            cleaned_name = clean_director_name(director)\n",
    "            name_map[cleaned_name].add(director)\n",
    "\n",
    "    return name_map\n",
    "\n",
    "\n",
    "# Step 3: Return the final director name map with cleaned versions\n",
    "director_name_map = create_name_map(df)\n",
    "\n",
    "# Display the result\n",
    "print(\"Director Name Map (cleaned name -> original names):\")\n",
    "dict(director_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df.loc[df[\"Director\"] == \"3 directors\", \"Director\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_name_map(df: pd.DataFrame):\n",
    "    name_map = defaultdict(set)  # Use a set to account for potential multiple matches\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        director_list = [name.strip() for name in row[\"Director\"].split(\",\")]\n",
    "\n",
    "        for director in director_list:\n",
    "            cleaned_name = clean_director_name(director)\n",
    "            name_map[cleaned_name].add(director)\n",
    "\n",
    "    # Filter to only show directors with more than 1 value\n",
    "    filtered_name_map = {key: value for key, value in name_map.items() if len(value) > 1}\n",
    "    return filtered_name_map\n",
    "\n",
    "\n",
    "# Create the final director name map\n",
    "director_name_map = create_name_map(df)\n",
    "\n",
    "# Display the result\n",
    "print(\"Directors with more than 1 value:\")\n",
    "director_name_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_title_counts = df.groupby(\"Description\")[\"Title\"].nunique()\n",
    "duplicate_descriptions = description_title_counts[description_title_counts > 1].index\n",
    "filtered_df = df[df[\"Description\"].isin(duplicate_descriptions)]\n",
    "\n",
    "filtered_df.sort_values(\"Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby([\"Title\", \"Description\"]).transform(\"size\")\n",
    "df[counts > 1].sort_values(\"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def plot_stopword_frequency(df: pd.DataFrame, column_name: str):\n",
    "    stopwords_in_titles = (\n",
    "        df[column_name].str.lower().str.split().apply(lambda x: [word for word in x if word in stop_words])\n",
    "    )\n",
    "    all_stopwords = [word for sublist in stopwords_in_titles for word in sublist]\n",
    "    stopword_counts = Counter(all_stopwords)\n",
    "\n",
    "    stopword_df = pd.DataFrame(stopword_counts.items(), columns=[\"Stopword\", \"Frequency\"]).sort_values(\n",
    "        by=\"Frequency\", ascending=False\n",
    "    )\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(stopword_df[\"Stopword\"], stopword_df[\"Frequency\"])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_stopword_frequency(df, \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "\n",
    "def check_significant_stopwords(df: pd.DataFrame, column_name: str):\n",
    "    titles_processed = df[column_name].str.lower().str.split()\n",
    "    stopword_df = pd.DataFrame(\n",
    "        {stopword: titles_processed.apply(lambda x: 1 if stopword in x else 0) for stopword in stop_words}\n",
    "    )\n",
    "    stopword_df = pd.concat([df[\"Genre\"], stopword_df], axis=1)\n",
    "\n",
    "    results = {}\n",
    "    for stopword in stop_words:\n",
    "        if stopword_df[stopword].sum() > 0:  # Only test stopwords that appear in at least one title\n",
    "            contingency_table = pd.crosstab(stopword_df[stopword], stopword_df[\"Genre\"])\n",
    "            chi2, p, dof, ex = chi2_contingency(contingency_table)\n",
    "            if p < 0.05:  # type: ignore\n",
    "                results[stopword] = (contingency_table, p)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "significant_stopwords = check_significant_stopwords(df, \"Title\")\n",
    "for stopword, (table, p_value) in significant_stopwords.items():\n",
    "    print(f\"Stopword: {stopword} P-value: {p_value}\\nCross-tabulation:\\n{table}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import numpy as np\n",
    "\n",
    "df.loc[:, \"Director\"] = df.loc[:, \"Director\"].replace(\"Unknown\", np.nan)\n",
    "df[\"Region\"] = df[\"From\"].map(\n",
    "    {\n",
    "        \"American\": \"Western\",\n",
    "        \"British\": \"Western\",\n",
    "        \"Canadian\": \"Western\",\n",
    "        \"Australian\": \"Western\",\n",
    "        \"Bollywood\": \"South Asian\",\n",
    "        \"Telugu\": \"South Asian\",\n",
    "        \"Tamil\": \"South Asian\",\n",
    "        \"Malayalam\": \"South Asian\",\n",
    "        \"Bengali\": \"South Asian\",\n",
    "        \"Kannada\": \"South Asian\",\n",
    "        \"Marathi\": \"South Asian\",\n",
    "        \"Punjabi\": \"South Asian\",\n",
    "        \"Assamese\": \"South Asian\",\n",
    "        \"Chinese\": \"East Asian\",\n",
    "        \"Japanese\": \"East Asian\",\n",
    "        \"South_Korean\": \"East Asian\",\n",
    "        \"Hong Kong\": \"East Asian\",\n",
    "        \"Filipino\": \"Southeast Asian\",\n",
    "        \"Bangladeshi\": \"South Asian\",\n",
    "        \"Russian\": \"European\",\n",
    "        \"Turkish\": \"Middle Eastern\",\n",
    "        \"Egyptian\": \"Middle Eastern\",\n",
    "        \"Malaysian\": \"Southeast Asian\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %autoreload 2\n",
    "# from src.dataset import preprocess_sentence\n",
    "\n",
    "# df[\"Title\"] = df[\"Title\"].apply(preprocess_sentence)\n",
    "# df[\"Description\"] = df[\"Description\"].apply(preprocess_sentence)\n",
    "\n",
    "# df.to_csv(\"data/processed/train.csv\", index=False)\n",
    "\n",
    "# df = pd.read_csv(\"data/processed/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from skrub import SelectCols, SimilarityEncoder\n",
    "\n",
    "text_pipeline = make_union(\n",
    "    make_pipeline(\n",
    "        ColumnSelector(\"Title\", drop_axis=True),\n",
    "        TfidfVectorizer(),\n",
    "        StandardScaler(with_mean=False),\n",
    "        TruncatedSVD(),\n",
    "    ),\n",
    "    make_pipeline(\n",
    "        ColumnSelector(\"Description\", drop_axis=True),\n",
    "        TfidfVectorizer(),\n",
    "        StandardScaler(with_mean=False),\n",
    "        TruncatedSVD(),\n",
    "    ),\n",
    "    make_pipeline(SelectCols(\"Region\"), SimilarityEncoder()),\n",
    "    make_pipeline(SelectCols(\"From\"), OneHotEncoder(sparse_output=False)),\n",
    ")\n",
    "\n",
    "pipeline = make_pipeline(text_pipeline, HistGradientBoostingClassifier())\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Genre\"] = label_encoder.fit_transform(df[\"Genre\"])\n",
    "\n",
    "# Split features and target\n",
    "X_train = df.drop(\"Genre\", axis=1)\n",
    "y_train = df[\"Genre\"]\n",
    "\n",
    "# Update param_distributions to match HistGradientBoostingClassifier\n",
    "param_distributions = {\n",
    "    \"histgradientboostingclassifier__learning_rate\": loguniform(0.01, 0.1),\n",
    "    \"histgradientboostingclassifier__max_iter\": randint(200, 450),\n",
    "    \"histgradientboostingclassifier__max_depth\": randint(4, 7),\n",
    "    \"histgradientboostingclassifier__min_samples_leaf\": randint(30, 100),\n",
    "    \"histgradientboostingclassifier__max_leaf_nodes\": randint(50, 150),\n",
    "    \"histgradientboostingclassifier__l2_regularization\": loguniform(1e-3, 0.1),\n",
    "    \"featureunion__pipeline-1__truncatedsvd__n_components\": randint(100, 250),\n",
    "    \"featureunion__pipeline-2__truncatedsvd__n_components\": randint(200, 500),\n",
    "}\n",
    "\n",
    "# Set up the RandomizedSearchCV with the updated parameters\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    cv=3,  # 5-fold cross-validation\n",
    "    scoring=\"accuracy\",  # Optimizing for accuracy\n",
    "    random_state=42,\n",
    "    verbose=4,\n",
    "    error_score=\"raise\",\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV with the updated pipeline\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters, score, and model\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "\n",
    "print(\"\\n Best Parameters:\")\n",
    "print(best_params)\n",
    "print(\"\\n Best Results:\")\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
